{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qingqingye/impact-of-network-6607-project-topic2/blob/main/dataclean_lda_6607project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import seaborn as sb\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from textblob import TextBlob\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "from bokeh.models import Label\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "sIWvo2KarTdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwIotbMOYrwh"
      },
      "outputs": [],
      "source": [
        "from operator import truediv\n",
        "from sqlite3 import DataError\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv(r'Expt3-ece6607-2022-fall modified.csv')\n",
        "dataframe = df[df['Your suggestions'].str.contains('\\n\\n')]\n",
        "dataframe = dataframe.reset_index(drop = True)\n",
        "\n",
        "dataframe['Before'] = dataframe['Your suggestions'].str.split('\\n\\n', expand = True)[0]\n",
        "dataframe['After'] = dataframe['Your suggestions'].str.split('\\n\\n', expand = True)[1]\n",
        "dataframe[\"Before\"]= dataframe[\"Before\"].str.split(\"\\n\", expand = False)\n",
        "dataframe[\"After\"]= dataframe[\"After\"].str.split(\"\\n\", expand = False)\n",
        "\n",
        "dataframe.to_csv('clean.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def get_top_n_words(n_top_words, count_vectorizer, text_data):\n",
        "    '''\n",
        "    returns a tuple of the top n words in a sample and their \n",
        "    accompanying counts, given a CountVectorizer object and text sample\n",
        "    '''\n",
        "    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n",
        "    vectorized_total = np.sum(vectorized_headlines, axis=0)\n",
        "    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n",
        "    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n",
        "    \n",
        "    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n",
        "    for i in range(n_top_words):\n",
        "        word_vectors[i,word_indices[0,i]] = 1\n",
        "\n",
        "    words = [word[0].encode('ascii').decode('utf-8') for \n",
        "             word in count_vectorizer.inverse_transform(word_vectors)]\n",
        "\n",
        "    return (words, word_values[0,:n_top_words].tolist()[0])"
      ],
      "metadata": {
        "id": "In5zlGL2e-Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import as_float_array\n",
        "# test_df = pd.read_csv('test.csv')\n",
        "# print(test_df)\n",
        "\n",
        "# reindexed_data = test_df['Before']\n",
        "# reindexed_data.index = test_df['Timestamp']\n",
        "\n",
        "\n",
        "dict = {'Type': [],\n",
        "        'Priority': [],\n",
        "        'Suggestion': [],\n",
        "        'index_num': []}\n",
        "for i in range(dataframe.shape[0]):\n",
        "  before_list = dataframe.loc[i, 'Before']\n",
        "  after_list = dataframe.loc[i, 'After']\n",
        "  min_len = min(len(before_list), len(after_list))\n",
        "  for i, suggestion in enumerate(before_list[: min_len]):\n",
        "    priority = i + 1\n",
        "    if len(suggestion) == 0:\n",
        "      continue\n",
        "    dict['Type'].append(0)\n",
        "    dict['Priority'].append(priority)\n",
        "    dict['Suggestion'].append(suggestion)\n",
        "  for i, suggestion in enumerate(after_list[: min_len]):\n",
        "    priority = i + 1\n",
        "    if len(suggestion) == 0:\n",
        "      continue\n",
        "    dict['Type'].append(1)\n",
        "    dict['Priority'].append(priority)\n",
        "    dict['Suggestion'].append(suggestion)\n",
        "dict['index_num'] = range(len(dict['Type']))\n",
        "\n",
        "indexed_data = pd.DataFrame(dict)\n",
        "indexed_data.dropna()\n",
        "indexed_data.to_csv('indexed_data.csv', index=False)\n",
        "print(indexed_data.shape[0])\n",
        "\n",
        "# count_vectorizer = CountVectorizer(stop_words='english')\n",
        "# words, word_values = get_top_n_words(n_top_words=10,\n",
        "#                                      count_vectorizer=count_vectorizer, \n",
        "#                                      text_data=reindexed_data)\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(16,8))\n",
        "# ax.bar(range(len(words)), word_values);\n",
        "# ax.set_xticks(range(len(words)));\n",
        "# ax.set_xticklabels(words, rotation='vertical');\n",
        "# ax.set_title('Top words in headlines dataset (excluding stop words)');\n",
        "# ax.set_xlabel('Word');\n",
        "# ax.set_ylabel('Number of occurences');\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "WgRCCDiarmKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca067a96-7cbd-4381-81e2-9d90914fbb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_indexed = pd.read_csv('indexed_data.csv')\n",
        "reindexed_data = df_indexed['Suggestion']\n",
        "reindexed_data.index = df_indexed['index_num']\n",
        "# reindexed_data.index = "
      ],
      "metadata": {
        "id": "NQQGlWhcFebo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\n",
        "small_text_sample = reindexed_data.sample(n=500, random_state=0).values\n",
        "\n",
        "print('Headline before vectorization: {}'.format(small_text_sample[0]))\n",
        "\n",
        "small_document_term_matrix = small_count_vectorizer.fit_transform(small_text_sample)\n",
        "\n",
        "print('Headline after vectorization: \\n{}'.format(small_document_term_matrix[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqKWRyk_vUxR",
        "outputId": "98c62279-85b6-44e2-efa6-d91ef1e21884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headline before vectorization: I think that the CULC roof should be open everyday.\n",
            "Headline after vectorization: \n",
            "  (0, 597)\t1\n",
            "  (0, 141)\t1\n",
            "  (0, 507)\t1\n",
            "  (0, 393)\t1\n",
            "  (0, 193)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_topics = 15"
      ],
      "metadata": {
        "id": "btaCWp73v6mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def get_keys(topic_matrix):\n",
        "    '''\n",
        "    returns an integer list of predicted topic \n",
        "    categories for a given topic matrix\n",
        "    '''\n",
        "    keys = topic_matrix.argmax(axis=1).tolist()\n",
        "    return keys\n",
        "\n",
        "def keys_to_counts(keys):\n",
        "    '''\n",
        "    returns a tuple of topic categories and their \n",
        "    accompanying magnitudes for a given list of keys\n",
        "    '''\n",
        "    count_pairs = Counter(keys).items()\n",
        "    categories = [pair[0] for pair in count_pairs]\n",
        "    counts = [pair[1] for pair in count_pairs]\n",
        "    return (categories, counts)"
      ],
      "metadata": {
        "id": "gZy4K59OwQcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def get_top_n_words_(n, keys, document_term_matrix, count_vectorizer):\n",
        "    '''\n",
        "    returns a list of n_topic strings, where each string contains the n most common \n",
        "    words in a predicted category, in order\n",
        "    '''\n",
        "    top_word_indices = []\n",
        "    for topic in range(n_topics):\n",
        "        temp_vector_sum = 0\n",
        "        for i in range(len(keys)):\n",
        "            if keys[i] == topic:\n",
        "                temp_vector_sum += document_term_matrix[i]\n",
        "        temp_vector_sum = temp_vector_sum.toarray()\n",
        "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
        "        top_word_indices.append(top_n_word_indices)   \n",
        "    top_words = []\n",
        "    for topic in top_word_indices:\n",
        "        topic_words = []\n",
        "        for index in topic:\n",
        "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
        "            temp_word_vector[:,index] = 1\n",
        "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
        "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
        "        top_words.append(\" \".join(topic_words))         \n",
        "    return top_words"
      ],
      "metadata": {
        "id": "Mlv1nIJswnxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', \n",
        "                                          random_state=0, verbose=0)\n",
        "lda_topic_matrix = lda_model.fit_transform(small_document_term_matrix)\n",
        "lda_keys = get_keys(lda_topic_matrix)\n",
        "lda_categories, lda_counts = keys_to_counts(lda_keys)\n",
        "top_n_words_lda = get_top_n_words_(8, lda_keys, small_document_term_matrix, small_count_vectorizer)\n",
        "\n",
        "for i in range(len(top_n_words_lda)):\n",
        "    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jxIigIqv8pG",
        "outputId": "d6ca2aa6-9f6b-419f-bdd6-d16a99dd7f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1:  increased campus pay gra opportunities employment number student\n",
            "Topic 2:  students free mental health campus access help provide\n",
            "Topic 3:  exams students open research book conceptual understanding able\n",
            "Topic 4:  options food campus course ece students better cheaper\n",
            "Topic 5:  student events fees lower library job support books\n",
            "Topic 6:  major students hands experience courses mindset crime improve\n",
            "Topic 7:  registration discounts course gas foods costco process early\n",
            "Topic 8:  course mode project person remote availability lectures wish\n",
            "Topic 9:  lower classes better housing students rent food quality\n",
            "Topic 10:  parking reduce campus fee traffic free affordable fees\n",
            "Topic 11:  discounted taxes employees speed improve student fi wi\n",
            "Topic 12:  opportunities ta assignments ra assignment exams weightage funding\n",
            "Topic 13:  courses students semester grad especially international create campus\n",
            "Topic 14:  better campus transportation affordable meal coverage flexible plans\n",
            "Topic 15:  campus increase safety activities opportunities gta gra students\n"
          ]
        }
      ]
    }
  ]
}